
# üìù Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">BMVC 2022</div><img src='images/IBSG.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[Improving Interpretability by Information Bottleneck Saliency Guided Localization](https://bmvc2022.mpi-inf.mpg.de/605/)\\
**<font style="background: #eeffee">Hao Zhou</font>**, Keyang Cheng, Yu Si, Liuyang Yan\\
*In this paper, we propose a deep learning interpretability method based on information bottleneck, which guides the model training by the probability distribution between the saliency map attributed by the information bottleneck and the gradient-based saliency map. This approach corrects the important regions focused by the model from an information-theoretic perspective. Meanwhile, a saliency suppression mechanism is presented to keep the saliency map of the model away from incorrect classification results and close to correct ones.*
  
<div style="height:auto;width:auto;overflow:auto;background-color:#def;scrollbar-base-color:gold;font-family:sans-serif;font-size:15px;padding:10px;overflow:auto;border:1px solid #abf;"><strong>Abstract</strong><br>
This paper presents Manual Manipulation and Decision Visualization (MMDV) which makes Human-in-the-loop improve the interpretability of deep neural networks. The MMDV offers three unique benefits: 1) The Expert-drawn CAM (Draw CAM) is presented to manipulate the key feature map and update the convolutional layer parameters, which makes the model focus on and learn the important parts by making a mask of the input image from the CAM drawn by the expert; 2) A hierarchical learning structure with sequential decision trees is proposed to provide a decision path and give strong interpretability for the fully connected layer of DNNs; 3) A novel metric, Data-Model-Result interpretable evaluation(DMR metric), is proposed to assess the interpretability of data, model and the results.
</div>
  
</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ACM MM 2022</div><img src='images/MMDV.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
[MMDV: Interpreting DNNs via Building Evaluation Metrics, Manual Manipulation and Decision Visualization](https://dl.acm.org/doi/abs/10.1145/3503161.3548260)\\
Keyang Cheng, Yu Si, **<font style="background: #eeffee">Hao Zhou</font>**, Rabia Tahir\\
*This paper presents Manual Manipulation and Decision Visualization (MMDV) which makes Human-in-the-loop improve the interpretability of deep neural networks. The MMDV offers three unique benefits: 1) The Expert-drawn CAM (Draw CAM) is presented to manipulate the key feature map and update the convolutional layer parameters, which makes the model focus on and learn the important parts by making a mask of the input image from the CAM drawn by the expert; 2) A hierarchical learning structure with sequential decision trees is proposed to provide a decision path and give strong interpretability for the fully connected layer of DNNs; 3) A novel metric, Data-Model-Result interpretable evaluation(DMR metric), is proposed to assess the interpretability of data, model and the results.* 
</div>
</div>

- ``Neurocomputing`` [Sonar Image Garbage Detection via Global Despeckling and Dynamic Attention Graph Optimization](https://www.sciencedirect.com/science/article/pii/S0925231223001133). \\Keyang Cheng, Liuyang Yan, Yi Ding, **<font style="background: #eeffee">Hao Zhou</font>**, Maozhen Li and Humaira abdul Ghafoor.
- ``ICPCSEE 2022`` [DRIB: Interpreting DNN with Dynamic Reasoning and Information Bottleneck](https://link.springer.com/chapter/10.1007/978-981-19-5194-7_14). \\Yu Si, Keyang Cheng, Zhou Jiang, **<font style="background: #eeffee">Hao Zhou</font>** and Rabia Tahir.
